# WordCount-Using-PySpark
The aim of this project is to process the word count on .txt file which is retrived from the external website and then visualizing that output in bar chart and word cloud.

## Tech Stack
- Pyspark API
- Word Cloud
- Python
- Spark Engine
- Databrics Cloud Environment

## Text Resource
[The Project Gutenberg eBook of Homecoming Horde, by Robert Silverberg](https://www.gutenberg.org/files/65119/65119-0.txt)

## Commands
### Data Importing
we are going to import urllib.request library in order to load the data from the url to Homecoming.txt.
```
import urllib.request
urllib.request.urlretrieve("https://www.gutenberg.org/files/65119/65119-0.txt" , "/tmp/Homecoming.txt")
```
Next we have to move that .txt file from temp folder to data folder i.e dbfs
In order to move that we have to use the dbutils.fs.mv
```
dbutils.fs.mv("file:/tmp/Homecoming.txt","dbfs:/data/Homecoming.txt")
```
Now we have to transfer that file to spark and then we have to covert our data into RDD
```
HomeComingRawRDD= sc.textFile("dbfs:/data/Homecoming.txt")
```
### Cleaning the data
In this step we are going to split the data by spaces and then converting that into lower case
```
HomeComingTokensRDD = HomeComingRawRDD.flatMap(lambda line: line.lower().strip().split(" "))
```
Next step remove the punctuations from the data by using regular expressions.
```
import re
HomeComingCleanTokensRDD = HomeComingTokensRDD.map(lambda letter: re.sub(r'[^A-Za-z]', '', letter))
```
Next step we are going to remove the stopwords present in the actual data for this we have to import the StopWordsRemover library from pyspark
```
from pyspark.ml.feature import StopWordsRemover
remover = StopWordsRemover()
stopwords = remover.getStopWords()
HomeComingWordsRDD = HomeComingCleanTokensRDD.filter(lambda PointLessW: PointLessW not in stopwords)

```
Next we are going to remove empty elements.
```
HomeComingEmptyRemoveRDD = HomeComingWordsRDD.filter(lambda x: x != "")
```
### Data Processing
In order to process the data we have to convert data into key-value pairs which is known as mapping like(word,1), here word is the actual word and one is the line value.
```
HomeComingPairsRDD = HomeComingEmptyRemoveRDD.map(lambda word: (word,1))
```
In next step we are going to implement the reducer face by this command.
```
HomeComingWordCountRDD = HomeComingPairsRDD.reduceByKey(lambda acc, value: acc + value)
```
Finally we are goint sort our list of words inorder to take the top ten items in our list ny using this command.
```
HomeComingResults = HomeComingWordCountRDD.map(lambda x: (x[1], x[0])).sortByKey(False).take(10)
2
print(HomeComingResults)
(3) Spark Jobs
```
### Charting the Result
In this we are going to visualize the wordCount output in bar chart by using pandas and MatPlotLib libraries.
```
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

source = 'The Project Gutenberg eBook of Homecoming Horde, by Robert Silverberg'
title = 'Top Words in ' + source
xlabel = 'Count'
ylabel = 'Words'

df = pd.DataFrame.from_records(HomeComingResults, columns =[xlabel, ylabel]) 
plt.figure(figsize=(10,3))
sns.barplot(xlabel, ylabel, data=df, palette="viridis").set_title(title)
```
![](https://github.com/SwaroopReddyGottigundala/pyspark-textprocessing-swaroop/blob/main/Screenshot%20(95).png?raw=true)
### Word Cloud
We can create the word cloud from the word count by importing nltk, wordcloud libraries.
```
import nltk
import wordcloud
import matplotlib.pyplot as plt

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud

class WordCloudGeneration:
    def preprocessing(self, data):
        # convert all words to lowercase
        data = [item.lower() for item in data]
        # load the stop_words of english
        stop_words = set(stopwords.words('english'))
        # concatenate all the data with spaces.
        paragraph = ' '.join(data)
        # tokenize the paragraph using the inbuilt tokenizer
        word_tokens = word_tokenize(paragraph) 
        # filter words present in stopwords list 
        preprocessed_data = ' '.join([word for word in word_tokens if not word in stop_words])
        print("\n Preprocessed Data: " ,preprocessed_data)
        return preprocessed_data

    def create_word_cloud(self, final_data):
        # initiate WordCloud object with parameters width, height, maximum font size and background color
        # call the generate method of WordCloud class to generate an image
        wordcloud = WordCloud(width=1600, height=800, max_words=10, max_font_size=200, background_color="black").generate(final_data)
        # plt the image generated by WordCloud class
        plt.figure(figsize=(12,10))
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.show()

wordcloud_generator = WordCloudGeneration()
# you may uncomment the following line to use custom input
# input_text = input("Enter the text here: ")
import urllib.request
url = "https://www.gutenberg.org/files/65119/65119-0.txt"
request = urllib.request.Request(url)
response = urllib.request.urlopen(request)
input_text = response.read().decode('utf-8')

input_text = input_text.split('.')
clean_data = wordcloud_generator.preprocessing(input_text)
wordcloud_generator.create_word_cloud(clean_data)
```
![](https://github.com/SwaroopReddyGottigundala/pyspark-textprocessing-swaroop/blob/main/Screenshot%20(94).png?raw=true)
### Insights
From the above word count charts we can conclude the important words of the story which are project, work, works, electronic.

## References
- [Word Cloud](https://www.section.io/engineering-education/word-cloud/)










